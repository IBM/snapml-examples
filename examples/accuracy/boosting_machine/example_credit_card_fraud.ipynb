{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Copyright 2021 IBM Corporation\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Machine on Credit Card Fraud Dataset\n",
    "\n",
    "## Background \n",
    "\n",
    "This is a classification problem to distinguish between a signal process which produces supersymmetric particles and a background process which does not.\n",
    "\n",
    "## Source\n",
    "\n",
    "Daniel Whiteson daniel '@' uci.edu, Assistant Professor, Physics & Astronomy, Univ. of California Irvine.\n",
    "\n",
    "In this example, we download the dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php).\n",
    "\n",
    "## Goal\n",
    "The goal of this notebook is to illustrate how Snap ML can accelerate training of a logistic regression model on this dataset.\n",
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T18:41:47.624289Z",
     "iopub.status.busy": "2021-03-01T18:41:47.623074Z",
     "iopub.status.idle": "2021-03-01T18:41:47.630882Z",
     "shell.execute_reply": "2021-03-01T18:41:47.631866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tpa/Code/snapml-examples/examples\n"
     ]
    }
   ],
   "source": [
    "cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T18:41:47.637476Z",
     "iopub.status.busy": "2021-03-01T18:41:47.636547Z",
     "iopub.status.idle": "2021-03-01T18:41:47.638989Z",
     "shell.execute_reply": "2021-03-01T18:41:47.639682Z"
    }
   },
   "outputs": [],
   "source": [
    "CACHE_DIR='cache-dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T18:41:47.644878Z",
     "iopub.status.busy": "2021-03-01T18:41:47.644123Z",
     "iopub.status.idle": "2021-03-01T18:41:50.150756Z",
     "shell.execute_reply": "2021-03-01T18:41:50.151221Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from datasets import CreditCardFraud\n",
    "from xgboost import XGBClassifier\n",
    "from snapml import BoostingMachineClassifier as SnapBoostingMachineClassifier\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T18:41:50.155800Z",
     "iopub.status.busy": "2021-03-01T18:41:50.155146Z",
     "iopub.status.idle": "2021-03-01T18:41:50.499188Z",
     "shell.execute_reply": "2021-03-01T18:41:50.499565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading binary CreditCardFraud dataset (cache) from disk.\n"
     ]
    }
   ],
   "source": [
    "dataset = CreditCardFraud(cache_dir=CACHE_DIR)\n",
    "X_train, X_test, y_train, y_test = dataset.get_train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T18:41:50.503412Z",
     "iopub.status.busy": "2021-03-01T18:41:50.502860Z",
     "iopub.status.idle": "2021-03-01T18:41:50.632471Z",
     "shell.execute_reply": "2021-03-01T18:41:50.632804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 213605\n",
      "Number of features: 28\n",
      "Number of classes:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of examples: %d\" % (X_train.shape[0]))\n",
    "print(\"Number of features: %d\" % (X_train.shape[1]))\n",
    "print(\"Number of classes:  %d\" % (len(np.unique(y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, PredefinedSplit\n",
    "train_ind, val_ind = train_test_split(range(0, X_train.shape[0]), test_size=0.3, shuffle=True, random_state=42)\n",
    "tmp = np.zeros(shape=(X_train.shape[0],))\n",
    "for i in train_ind:\n",
    "    tmp[i] = -1\n",
    "splitter = PredefinedSplit(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from sklearn.utils import parallel_backend\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.5008652385150725, 1: 289.43766937669375}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "\n",
    "class_weights = {\n",
    "    0: y_train.shape[0]/2.0/np.sum(y_train == 0),\n",
    "    1: y_train.shape[0]/2.0/np.sum(y_train == 1)\n",
    "}\n",
    "print(class_weights)\n",
    "\n",
    "w_train = compute_sample_weight(class_weights, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def weighted_neg_log_loss(y, p):\n",
    "    w = compute_sample_weight(class_weights, y)\n",
    "    return -log_loss(y, p.astype(np.float64), sample_weight=w)\n",
    "\n",
    "scorer = make_scorer(weighted_neg_log_loss, greater_is_better=True, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_params = {\n",
    "    'n_candidates': 256,\n",
    "    'min_resources': 16,\n",
    "    'max_resources': 1024,\n",
    "    'factor': 4,\n",
    "    'scoring': scorer,\n",
    "    'random_state': 42,\n",
    "    'verbose': 40,\n",
    "    'n_jobs': 4,\n",
    "    'cv': splitter,\n",
    "    'return_train_score': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 4\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 16\n",
      "max_resources_: 1024\n",
      "aggressive_elimination: False\n",
      "factor: 4\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 256\n",
      "n_resources: 16\n",
      "Fitting 1 folds for each of 256 candidates, totalling 256 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 64\n",
      "n_resources: 64\n",
      "Fitting 1 folds for each of 64 candidates, totalling 64 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 16\n",
      "n_resources: 256\n",
      "Fitting 1 folds for each of 16 candidates, totalling 16 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 4\n",
      "n_resources: 1024\n",
      "Fitting 1 folds for each of 4 candidates, totalling 4 fits\n",
      "{'subsample': 0.6326530612244898, 'reg_lambda': 56.89866029018293, 'max_depth': 1, 'learning_rate': 0.09319395762340775, 'colsample_bytree': 0.7142857142857143, 'n_estimators': 1024}\n",
      "274.5722529888153 -0.4541671767181547 -0.26632982394058535\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(random_state=42, \n",
    "                    n_jobs=1,\n",
    "                    tree_method='hist',\n",
    "                    use_label_encoder=False,\n",
    "                    eval_metric='logloss',\n",
    "                    max_bin=256)\n",
    "\n",
    "xgb_distributions = {\n",
    "    \"max_depth\": range(1, 20),\n",
    "    \"learning_rate\": 10 ** np.linspace(-2.5, -1),\n",
    "    \"colsample_bytree\": np.linspace(0.5, 1.0),\n",
    "    \"subsample\": np.linspace(0.5, 1.0),\n",
    "    \"reg_lambda\": 10 ** np.linspace(-2, 2)\n",
    "}\n",
    "\n",
    "search = HalvingRandomSearchCV(clf, xgb_distributions, resource='n_estimators', **sh_params)\n",
    "                        \n",
    "t0 = time.time()\n",
    "with parallel_backend(\"loky\"): \n",
    "    search.fit(X_train, y_train.astype(np.int32), sample_weight=w_train)\n",
    "t_fit_xgb  = time.time()-t0\n",
    "\n",
    "\n",
    "print(search.best_params_)\n",
    "\n",
    "score_xgb = weighted_neg_log_loss(y_test, search.predict_proba(X_test)[:,1])\n",
    "\n",
    "print(t_fit_xgb, search.best_score_, score_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 4\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 16\n",
      "max_resources_: 1024\n",
      "aggressive_elimination: False\n",
      "factor: 4\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 256\n",
      "n_resources: 16\n",
      "Fitting 1 folds for each of 256 candidates, totalling 256 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tpa/anaconda3/envs/snapenv/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "iter: 1\n",
      "n_candidates: 64\n",
      "n_resources: 64\n",
      "Fitting 1 folds for each of 64 candidates, totalling 64 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 16\n",
      "n_resources: 256\n",
      "Fitting 1 folds for each of 16 candidates, totalling 16 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 4\n",
      "n_resources: 1024\n",
      "Fitting 1 folds for each of 4 candidates, totalling 4 fits\n",
      "{'tree_select_probability': 0.9346938775510204, 'subsample': 0.846938775510204, 'regularizer': 6.250551925273976, 'n_components': 58, 'max_depth': 1, 'learning_rate': 0.08094001216083124, 'lambda_l2': 56.89866029018293, 'gamma': 568.9866029018293, 'fit_intercept': True, 'colsample_bytree': 0.5408163265306123, 'num_round': 1024}\n",
      "275.0033118724823 -0.45589279056285664 -0.25351774172662755\n"
     ]
    }
   ],
   "source": [
    "clf = SnapBoostingMachineClassifier(random_state=42, n_jobs=1, base_score=None)\n",
    "\n",
    "snap_distributions = {\n",
    "    \"max_depth\": range(1, 20),\n",
    "    \"tree_select_probability\": np.linspace(0.9, 1.0),\n",
    "    \"learning_rate\": 10 ** np.linspace(-2.5, -1),\n",
    "    \"colsample_bytree\": np.linspace(0.5, 1.0),\n",
    "    \"subsample\": np.linspace(0.5, 1.0),\n",
    "    \"lambda_l2\": 10 ** np.linspace(-2, 2),\n",
    "    \"regularizer\": 10 ** np.linspace(-6, 3),\n",
    "    \"fit_intercept\": [False, True],\n",
    "    \"gamma\": 10 ** np.linspace(-3, 3),\n",
    "    \"n_components\": range(1, 100)   \n",
    "}\n",
    "\n",
    "search = HalvingRandomSearchCV(clf, snap_distributions, resource='num_round', **sh_params)\n",
    "                             \n",
    "t0 = time.time()\n",
    "with parallel_backend(\"loky\"): \n",
    "    search.fit(X_train, y_train, sample_weight=w_train)\n",
    "t_fit_snapml = time.time()-t0\n",
    "\n",
    "print(search.best_params_)\n",
    "\n",
    "score_snapml = weighted_neg_log_loss(y_test, search.predict_proba(X_test)[:,1])\n",
    "\n",
    "print(t_fit_snapml, search.best_score_, score_snapml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T18:43:37.012926Z",
     "iopub.status.busy": "2021-03-01T18:43:37.012424Z",
     "iopub.status.idle": "2021-03-01T18:43:37.014286Z",
     "shell.execute_reply": "2021-03-01T18:43:37.014759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed-up:                1.0 x\n",
      "Relative diff. in score: 0.0481\n"
     ]
    }
   ],
   "source": [
    "speed_up = t_fit_xgb/t_fit_snapml\n",
    "score_diff = (score_snapml-score_xgb)/np.abs(score_xgb)\n",
    "print(\"Speed-up:                %.1f x\" % (speed_up))\n",
    "print(\"Relative diff. in score: %.4f\" % (score_diff))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
